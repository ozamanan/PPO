{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team\n",
    "Manan Oza\n",
    "\n",
    "Sachin Venugopal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization on CartPole-v0 and CartPole-v1\n",
    "\n",
    "## 1.1 Motivation\n",
    "\n",
    "#### The motivation of this project is to train a model to learn to play Cartpole-v0 using the Proximal Policy Optimization technique and to show the significant improvement in performance over 2 other commonly used algorithms. \n",
    "\n",
    "## 1.2 Proximal Policy Optimization\n",
    "\n",
    "Policy Gradient methods have convergence problem which is addressed by the natural policy gradient. To improve training stability, we should avoid parameter updates that change the policy too much in one step. The Trust Region Policy Optimization and Proximal Policy Optimization are techniques that carry out this idea by enforcing a KL divergence constraint on the size of the policy update at each iteration. \n",
    "\n",
    "{Reference: https://spinningup.openai.com/en/latest/algorithms/ppo.html, https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12}\n",
    "\n",
    "### Trust Region \n",
    "\n",
    "In the trust region method, we determine the maximum step size that we want to explore first. Then we locate the optimal point within the trust region and resume the search from there. In order not to make bad decisions, we can shrink the trust region if the policy is changing too much. Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.\n",
    "\n",
    "\n",
    "\n",
    "There are two primary variants of PPO:\n",
    "   1. $\\textbf{PPO-Penalty}$ approximately solves a KL-constrained update like TRPO, but penalizes the KL-divergence in the objective function instead of making it a hard constraint, and automatically adjusts the penalty coefficient over the course of training so that it’s scaled appropriately.\n",
    "   2. $\\textbf{PPO-Clip} $ doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    "\n",
    "PPO algorithm is as follows:\n",
    "\n",
    "<img src=\"algo.png\">\n",
    "*Image taken from lecture slides.\n",
    "\n",
    "\n",
    "### KL Divergence \n",
    "\n",
    "The Kullback-Leibler (KL) Divergence score quantifies how much one probability distribution differs from another probability distribution. In PPO, we limit how far we can change our policy in each iteration through the KL-divergence. \n",
    "\n",
    "> The KL Divergence between two distributions Q and P is :\n",
    "    $ D_{KL} (P||Q) = E_{x}log\\dfrac{P(x)}{Q(x)}$\n",
    "\n",
    "We re-purpose it in PPO to measure the difference between any two policies. We don't want the new policy to be too different from the current one.\n",
    "\n",
    "\n",
    "### Cartpole-v0\n",
    "This environment consists of a pole attached to a cart which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright and the goal is to to prevent it from falling over( more than 15 degrees from vertical) while making sure that the cart does not move more than 2.4 units from the center. A reward of +1 is provided for every timestep the pole remains upright. The environment times out and terminates at the end of 200 timesteps.\n",
    "\n",
    "\n",
    "    Actions: [0 1] where 0 => Move left 1 => Move right\n",
    "\n",
    "    Observations: [0 1 2 3] where 0 => cart position, min = -2.4, max = -2.4 1 => Cart Velocity, min = -inf, max = inf 2 => Pole \n",
    "\n",
    "    Angle, min ~-41.8 degrees, max = ~41.8 degrees 3 => Pole Velocity At tip, min = -inf, max = inf\n",
    "\n",
    "    Reward: +1 for every time step the pole is vertical, max = 200\n",
    "    The stopping condition is a moving 100 episode reward average of 195\n",
    "    \n",
    "### Cartpole-v1\n",
    "This environment consists of a pole attached to a cart which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright and the goal is to to prevent it from falling over( more than 15 degrees from vertical) while making sure that the cart does not move more than 2.4 units from the center. A reward of +1 is provided for every timestep the pole remains upright. The environment times out and terminates at the end of 500 timesteps.\n",
    "\n",
    "\n",
    "    Actions: [0 1] where 0 => Move left 1 => Move right\n",
    "\n",
    "    Observations: [0 1 2 3] where 0 => cart position, min = -2.4, max = -2.4 1 => Cart Velocity, min = -inf, max = inf 2 => Pole \n",
    "\n",
    "    Angle, min ~-41.8 degrees, max = ~41.8 degrees 3 => Pole Velocity At tip, min = -inf, max = inf\n",
    "\n",
    "    Reward: +1 for every time step the pole is vertical, max = 500\n",
    "    The stopping condition is a moving 100 episode reward average of 475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we define the PPO loss function that we will use while compliling the actor model.\n",
    "### This loss function calculates the clipped advantage value and adds the clipped entropy.\n",
    "### The entropy helps in regulating exploitation and exploration.\n",
    "### The agents gets penalized for lesser exploration.\n",
    "\n",
    "def ppo_loss_func(advantage, prev_pred, clip_loss = 0.2, clip_entropy = 5e-3):\n",
    "    def loss(y_true, y_pred):\n",
    "        ratio = K.sum(y_true * y_pred, axis = -1) / (K.sum(y_true * prev_pred, axis = -1) + 1e-7)\n",
    "        probability = K.sum(y_true * y_pred, axis = -1)\n",
    "        clipped_loss = -K.mean(K.minimum(ratio * advantage, K.clip(ratio, min_value = (1 - clip_loss), \n",
    "                    max_value = (1 + clip_loss)) * advantage) + clip_entropy * -(probability\n",
    "                    * K.log(probability + 1e-7)))\n",
    "        return clipped_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agent Class\n",
    "class Agent():\n",
    "    def __init__(self, env, action_space, state_space, learning_rate, buffer_len, gamma):\n",
    "        self.env = env\n",
    "        self.reward = []\n",
    "        self.observation = self.env.reset()\n",
    "        self.learning_rate = learning_rate ### learning rate\n",
    "        self.buffer_len = buffer_len ### The length of buffer which stores previous results from which we \n",
    "        self.gamma = gamma  ### discount factor\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.actor = self.build_actor_network()\n",
    "        self.critic = self.build_critic_network()\n",
    "    \n",
    "    def build_actor_network(self): ### builds the actor network\n",
    "        input_state = Input(shape=(self.state_space,))\n",
    "        advantage = Input(shape=(1,))\n",
    "        pred = Input(shape=(self.action_space,))\n",
    "        \n",
    "        layer_1 = Dense(128, activation='tanh')(input_state)\n",
    "        layer_2 = Dense(128, activation='tanh')(layer_1)\n",
    "        out = Dense(self.action_space, activation='softmax')(layer_2)\n",
    "        \n",
    "        actor_model = Model(inputs=[input_state, advantage, pred], outputs=[out])\n",
    "        actor_model.compile(optimizer=Adam(lr=self.learning_rate), \n",
    "                      loss=[ppo_loss_func(advantage=advantage, prev_pred=pred)]) ### here we use the \n",
    "                                                                            ### loss function defined by us earlier\n",
    "        \n",
    "        actor_model.summary()\n",
    "        return actor_model\n",
    "    \n",
    "    def build_critic_network(self): ### builds the critic network\n",
    "        input_state = Input(shape=(self.state_space,))\n",
    "        \n",
    "        layer_1 = Dense(128, activation='tanh')(input_state)\n",
    "        layer_2 = Dense(128, activation='tanh')(layer_1)\n",
    "        out = Dense(1)(layer_2)\n",
    "        \n",
    "        critic_model = Model(inputs=[input_state], outputs=[out])\n",
    "        critic_model.compile(optimizer=Adam(lr=self.learning_rate), loss='mse')\n",
    "        \n",
    "        critic_model.summary()\n",
    "        return critic_model\n",
    "    \n",
    "    def discounted_reward(self):\n",
    "        a = np.array(self.reward).sum()\n",
    "        for t in range(len(self.reward) - 2, -1, -1):\n",
    "            self.reward[t] += self.reward[t + 1] * self.gamma ### multiplies gamma^(T-t+1) with reward[t+1]\n",
    "                                                              ### i.e. discounted reward\n",
    "        return a\n",
    "    \n",
    "    def fetch_act(self):\n",
    "        predicted_action = self.actor.predict([self.observation.reshape(1, 4), \n",
    "                                               np.zeros((1, 1)), np.zeros((1, 2))]) ### predicts which action \n",
    "                                                            ### needs to be taken based in the current observation\n",
    "        action_taken = np.random.choice(2, p=np.nan_to_num(predicted_action[0])) ### the performed action\n",
    "        action_mat = np.zeros(2) ### makes a matrix of two '0' values and assigns '1' to the \n",
    "                                ### corresponding action that has been performed\n",
    "        action_mat[action_taken] = 1\n",
    "        return action_taken, action_mat, predicted_action\n",
    "    \n",
    "    def get_batch(self): ### used to make batches\n",
    "        batch = [[], [], [], []]\n",
    "\n",
    "        per_episode_reward = []\n",
    "        observation_list, action_list, pred_list = [], [], []\n",
    "        while len(batch[0]) < self.buffer_len:\n",
    "            action, action_matrix, predicted_action = self.fetch_act()\n",
    "            observation, reward, done, info = self.env.step(action) ### Performs the said action\n",
    "#             self.env.render()  ### Uncomment to render the environment while training\n",
    "            self.reward.append(reward)\n",
    "\n",
    "            observation_list.append(self.observation) ### list of all observations\n",
    "            action_list.append(action_matrix) ### list of all actions performed\n",
    "            pred_list.append(predicted_action) ### list of all predicted actions\n",
    "            self.observation = observation\n",
    "            \n",
    "            ### This for loop creates batches of the observations, actions taken, \n",
    "            ### predicted actions and the corresponding rewards\n",
    "            ### These batches are then used to train the actor and critic networks.\n",
    "            if done == True:\n",
    "                per_episode_reward.append(self.discounted_reward())\n",
    "                for i in range(len(self.reward)):\n",
    "                    obs_batch = observation_list[i]\n",
    "                    action_batch = action_list[i]\n",
    "                    pred_batch = pred_list[i]\n",
    "                    reward_batch = self.reward[i]\n",
    "                    batch[0].append(obs_batch)\n",
    "                    batch[1].append(action_batch)\n",
    "                    batch[2].append(pred_batch)\n",
    "                    batch[3].append(reward_batch)\n",
    "                observation_mat, action_mat, pred_mat = [], [], []\n",
    "                self.observation = self.env.reset()\n",
    "                self.reward = []\n",
    "        \n",
    "        obs_array = np.array(batch[0])\n",
    "        action_array = np.array(batch[1])\n",
    "        predict_array = np.array(batch[2])\n",
    "        reward_array = np.reshape(np.array(batch[3]), (len(batch[3]), 1))\n",
    "        \n",
    "        predict_array = np.reshape(predict_array, (predict_array.shape[0], predict_array.shape[2]))\n",
    "        return obs_array, action_array, predict_array, reward_array, per_episode_reward ### returns the batches \n",
    "                                                                                ### in the form of numpy arrays\n",
    "\n",
    "\n",
    "    def train(self, max_episodes, epochs, batch_size): ### trainer function\n",
    "        reward_list = []\n",
    "        actor_history_list = []\n",
    "        critic_history_list = []\n",
    "        moving_avg = []\n",
    "        \n",
    "        for episode in range(max_episodes):\n",
    "            obs, action, pred, reward, per_episode_reward = self.get_batch() ### generates a batch of observations,\n",
    "                                                                    ### actions, predicted actions and rewards\n",
    "            print(\"Episode = \" + str(episode) + \" || average reward = \" + str(np.mean(per_episode_reward)))\n",
    "            obs = obs[:self.buffer_len]\n",
    "            pred = pred[:self.buffer_len]\n",
    "            action = action[:self.buffer_len]\n",
    "            reward = reward[:self.buffer_len]\n",
    "            \n",
    "            prev_pred = pred\n",
    "            state_values = self.critic.predict(obs) ### gets output from critic (state values)\n",
    "            advantage = reward - state_values ### calculates the advantage\n",
    "            \n",
    "            reward_list.append(np.mean(per_episode_reward))\n",
    "            if episode <= 99:\n",
    "                moving_avg.append(0)\n",
    "            else:\n",
    "                moving_avg.append(np.mean(reward_list))\n",
    "            \n",
    "            ### trains the actor network for epochs = 10\n",
    "            self.actor_loss = self.actor.fit([obs, advantage, prev_pred], [action], \n",
    "                            batch_size = batch_size, shuffle=True, epochs=epochs, verbose = 0)\n",
    "            \n",
    "            ### trains the critic network for epochs = 10\n",
    "            self.critic_loss = self.critic.fit([obs], [reward], \n",
    "                                batch_size=batch_size, shuffle=True, epochs=epochs, verbose = 0)\n",
    "            \n",
    "            actor_history_list = actor_history_list + self.actor_loss.history['loss']\n",
    "            critic_history_list = critic_history_list + self.critic_loss.history['loss']\n",
    "        \n",
    "        ### plots rewards vs episodes\n",
    "        plt.plot(reward_list)\n",
    "        plt.plot(moving_avg)\n",
    "        plt.title('Rewards')\n",
    "        plt.ylabel('Reward per episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.legend(['Per episode reward', '100 episode moving reward'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        ### plots loss for actor network\n",
    "        plt.plot(actor_history_list)\n",
    "        plt.title('Actor loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        ### plots loss for critic network\n",
    "        plt.plot(critic_history_list)\n",
    "        plt.title('Critic loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env):\n",
    "    env = gym.make(env)\n",
    "    max_episodes = 1000\n",
    "    epochs = 10 ### train the networks for 10 epochs at one time.\n",
    "    gamma = 0.99\n",
    "    buffer_len = 2048\n",
    "    batch_size = 256\n",
    "    action_space = env.action_space.n\n",
    "    state_space = env.observation_space.shape[0]\n",
    "    learning_rate = 1e-4\n",
    "    agent = Agent(env, action_space, state_space, learning_rate, buffer_len, gamma)\n",
    "    agent.train(max_episodes, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = \"CartPole-v1\"\n",
    "    main(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute this to test the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import gym\n",
    "import keras \n",
    "from gym import wrappers, logger\n",
    "# Test the PPO trained model for CartPole-v0\n",
    "env = gym.make('CartPole-v0') ### change to 'CartPole-v1' \n",
    "env.reset()\n",
    "model = load_model(\"cartpole_v0_ppo\") ### change file according to the environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "final_reward = 0\n",
    "while not Done: # can be set to while True for an infnitely long episode\n",
    "    env.render()\n",
    "    state = np.expand_dims(np.array(state), axis=0)\n",
    "    action = model.predict(state)\n",
    "    next_state, reward, done, info = env.step(action=np.argmax(action))\n",
    "    state = next_state\n",
    "    final_reward += reward\n",
    "env.close()\n",
    "print(final_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS\n",
    "## PPO Losses\n",
    "#### CartPole-v0\n",
    "<img src=\"ppocartpolev0losses.png\">\n",
    "\n",
    "#### CartPole-v1\n",
    "<img src=\"ppocartpolev1losses.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with DDQN and DQN\n",
    "## CartPole-v0\n",
    "### PPO\n",
    "<img src=\"ppocartpolev0rewards.png\">\n",
    "\n",
    "### DDQN\n",
    "<img src=\"ddqncartpole0.png\">\n",
    "\n",
    "### DQN\n",
    "<img src=\"dqncartpolev0.png\">\n",
    "\n",
    "## CartPole-v1\n",
    "### PPO\n",
    "<img src=\"ppocartpolev1rewards.png\">\n",
    "\n",
    "### DDQN\n",
    "<img src=\"ddqncartpolev1.png\">\n",
    "\n",
    "### DQN\n",
    "<img src=\"dqncartpolev1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above graphs of rewards vs episodes we can easily conclude that PPO gives us better training results and also makes the training stable as compared to DDQN and DQN. The PPO agent learns faster than the other two algorithms and also consistently learns whereas the DDQN and DQN agents have a noisy learning curve and hence the plot of rewards are quite noisy. The PPO algorithm is therefore a much better learning algorithm that gives robust results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
